{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb467af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary python packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load dataset\n",
    "olddata = pd.read_csv(\"MLolddata .csv\")\n",
    "data = pd.read_csv(\"MLdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326aa49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old dataset size: (1149, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    427\n",
       "1    406\n",
       "0    316\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The old dataset size\n",
    "print('old dataset size:',olddata.shape)\n",
    "\n",
    "# Counts the occurrences of each unique label in the old dataset\n",
    "olddata['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d27a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "num_duplicates = data.duplicated().sum()\n",
    "num_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1afb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db559ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary  \n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import ISRIStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18e459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arabic stopwords\n",
    "arabic_stopwords = set(stopwords.words('arabic'))\n",
    "\n",
    "# Stemmer for Arabic words\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "\n",
    "def remove_special(text):\n",
    "    for letter in '#.][!XR':\n",
    "        text = text.replace(letter, '')\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "    english_punctuations = string.punctuation\n",
    "    punctuations_list = arabic_punctuations + english_punctuations\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def clean_str(text):\n",
    "    search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\", '\\n', '\\t'\", '\"', '?', '؟', '!']\n",
    "    replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ', ' ؟ ', ' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "\n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def keep_only_arabic(text):\n",
    "    return re.sub(r'[a-zA-Z?]', '', text).strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_special(text)\n",
    "    text = remove_punctuations(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = clean_str(text)\n",
    "    text = keep_only_arabic(text)\n",
    "    \n",
    "    tokens = [word for word in text.split() if word not in arabic_stopwords]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb7c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60a705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features and labels\n",
    "X = tfidf_vectorizer.fit_transform(data['text'])\n",
    "y = data['Lable']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bee7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics using default parameters:\n",
      "\n",
      "Training Accuracy: 0.9291\n",
      "Testing Accuracy: 0.6883\n",
      "Precision: 0.6914\n",
      "Recall: 0.6883\n",
      "F1-score: 0.6887\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define the naive bayes model\n",
    "naive_model = MultinomialNB()\n",
    "\n",
    "# Fit the model on the training data\n",
    "naive_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance using default parameter values\n",
    "print(\"Performance metrics using default parameters:\\n\")\n",
    "print(f\"Training Accuracy: {naive_model.score(X_train, y_train):.4f}\")\n",
    "print(f\"Testing Accuracy: {naive_model.score(X_test, y_test):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, naive_model.predict(X_test), average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, naive_model.predict(X_test), average='weighted'):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test, naive_model.predict(X_test), average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c45a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value of each parameter: {'alpha': 2.0}\n",
      "Best cross-validation score: 0.6565246372352972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "rr_scaler = RobustScaler(with_centering=False)\n",
    "x_rr =rr_scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(x_rr, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid4 = {'alpha': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]} \n",
    "\n",
    "grid_search4 = GridSearchCV(MultinomialNB(), param_grid4, cv=5)\n",
    "\n",
    "grid_search4.fit(X_train2, y_train2)\n",
    "\n",
    "# print the optimal value of each parameter\n",
    "print(\"optimal value of each parameter:\",grid_search4.best_params_)\n",
    "print(\"Best cross-validation score:\",grid_search4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0df9b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance after tuning the parameters:\n",
      "\n",
      "Training Accuracy: 0.9068\n",
      "Testing Accuracy: 0.7004\n",
      "Precision: 0.7035\n",
      "Recall: 0.7004\n",
      "F1-score: 0.7007\n"
     ]
    }
   ],
   "source": [
    "# Rebuild a model on the training set using the optimum parameters' values\n",
    "# evaluate the model on the test set\n",
    "re_naive = grid_search4.best_estimator_\n",
    "re_naive.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluate model performance after tuning the parameters\n",
    "print(\"performance after tuning the parameters:\\n\")\n",
    "print(f\"Training Accuracy: {re_naive.score(X_train2, y_train2):.4f}\")\n",
    "print(f\"Testing Accuracy: {re_naive.score(X_test2, y_test2):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test2, re_naive.predict(X_test2), average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test2, re_naive.predict(X_test2), average='weighted'):.4f}\")\n",
    "print(f\"F1-score: {f1_score(y_test2, re_naive.predict(X_test2), average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e552d072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(re_naive, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4209aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: altair>=3.2.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (4.0.0)\n",
      "Requirement already satisfied: tornado>=5.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (4.8.1)\n",
      "Requirement already satisfied: pympler>=0.9 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (1.0.1)\n",
      "Requirement already satisfied: semver in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (3.0.2)\n",
      "Requirement already satisfied: cachetools>=4.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: requests>=2.4 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (2.26.0)\n",
      "Requirement already satisfied: toml in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: packaging>=14.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (21.0)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (16.0.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (5.2)\n",
      "Requirement already satisfied: validators>=0.2 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (0.28.1)\n",
      "Requirement already satisfied: numpy in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (1.20.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (8.4.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (8.0.3)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (0.9.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: jinja2 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
      "Requirement already satisfied: toolz in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
      "Requirement already satisfied: jsonschema in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from altair>=3.2.0->streamlit) (3.2.0)\n",
      "Requirement already satisfied: entrypoints in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from altair>=3.2.0->streamlit) (0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from gitpython!=3.1.19->streamlit) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (5.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=1.4->streamlit) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from packaging>=14.1->streamlit) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.21.0->streamlit) (2021.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from jinja2->altair>=3.2.0->streamlit) (1.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil->streamlit) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.4->streamlit) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.4->streamlit) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.4->streamlit) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.4->streamlit) (2021.10.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from rich>=10.11.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from rich>=10.11.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->altair>=3.2.0->streamlit) (21.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->altair>=3.2.0->streamlit) (58.0.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (from jsonschema->altair>=3.2.0->streamlit) (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00c5700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: git-lfs in /Users/leenharbi/opt/anaconda3/lib/python3.9/site-packages (1.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249ea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b0a2105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# تهيئة وتدريب متجه TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# حفظ المتجه TF-IDF\n",
    "import joblib\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f7bcf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile my_streamlit_app.py\n",
    "import streamlit as st\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "# تحميل النموذج المدرب والمتجه\n",
    "model = joblib.load(\"model.pkl\")\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# تحديد العناصر الواجهة\n",
    "st.title(\"Classifying sentiments \")\n",
    "text_input = st.text_input(\"Please enter text:\")\n",
    "\n",
    "# التنبؤ بالتصنيف\n",
    "if text_input:\n",
    "    # تحويل النص إلى متجه TF-IDF\n",
    "    text_vectorized = vectorizer.transform([text_input])\n",
    "    # التنبؤ باستخدام النموذج\n",
    "    prediction = model.predict(text_vectorized)\n",
    "    # عرض نتيجة التنبؤ\n",
    "    if prediction == 1:\n",
    "        st.write(\"Positive: إيجابي\")\n",
    "    elif prediction == -1:\n",
    "        st.write(\"Negative: سلبي\")\n",
    "    else:\n",
    "        st.write(\"Neutral: طبيعي\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d9dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
